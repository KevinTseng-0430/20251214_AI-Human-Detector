import re

import altair as alt
import numpy as np
import pandas as pd
import streamlit as st
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline


def build_dataset():
    """Return small, hand-crafted samples for AI-like and human-like writing."""
    ai_like_en = [
        "This article explores the impact of artificial intelligence on healthcare systems. In conclusion, data-driven triage pipelines will continue to mature.",
        "The model achieves strong accuracy according to our evaluation, demonstrating scalability and consistency across benchmarks.",
        "Overall, the findings suggest that energy efficiency will remain a key metric for sustainable infrastructure planning.",
        "In this section we formalize the problem, introduce the methodology, and discuss limitations of our approach.",
        "The results are summarized in Table 2 and indicate robust performance even under constrained memory settings.",
        "We recommend adopting a phased rollout strategy with clear monitoring criteria and periodic audits.",
        "Future work can extend this framework to multilingual corpora without significant architectural changes.",
        "The experiment was replicated three times to reduce variance, and the confidence intervals are reported accordingly.",
        "This overview highlights how modular design promotes maintainability, reproducibility, and traceability of decisions.",
        "In conclusion, these observations underscore the importance of aligning incentives with measurable outcomes.",
        "The baseline uses deterministic heuristics, while the proposed system integrates probabilistic reasoning and calibration.",
        "We adopt a minimal set of hyperparameters to simplify deployment and reduce operational overhead."
    ]

    ai_like_zh = [
        "æœ¬ç ”ç©¶æ¢è¨å¤§å‹èªè¨€æ¨¡å‹åœ¨å®¢æœè‡ªå‹•åŒ–çš„æ•ˆèƒ½ï¼Œå¯¦é©—çµæœé¡¯ç¤ºæº–ç¢ºç‡èˆ‡æˆæœ¬çš†æœ‰é¡¯è‘—æå‡ã€‚",
        "æ•´é«”æµç¨‹æ¡åˆ†éšæ®µéƒ¨ç½²ä¸¦åŠ å…¥å³æ™‚ç›£æ§ï¼Œä»¥ç¢ºä¿æ¨¡å‹è¡¨ç¾ç©©å®šä¸”å¯è¿½è¹¤ã€‚",
        "æ•¸æ“šæ¨™è¨»ç¶“éé›™é‡å¯©æ ¸ï¼Œä¸¦ä»¥ä¸€è‡´æ€§æŒ‡æ¨™ç¢ºèªå“è³ªå¾Œæ‰é€²å…¥è¨“ç·´æµç¨‹ã€‚",
        "æˆ‘å€‘ä½¿ç”¨å¤šèªå¹³è¡¡èªæ–™é€²è¡Œå¾®èª¿ï¼Œä»¥é™ä½åå·®ä¸¦æå‡è·¨é ˜åŸŸçš„æ³›åŒ–èƒ½åŠ›ã€‚",
        "å¯¦é©—é‡è¤‡ä¸‰æ¬¡ä»¥é™ä½éš¨æ©Ÿæ€§ï¼Œè¡¨ 2 åˆ—å‡ºå„é …æŒ‡æ¨™çš„ä¿¡è³´å€é–“èˆ‡è®Šç•°ç¨‹åº¦ã€‚",
        "çµè«–å¼·èª¿æ¨¡å‹éœ€æ­é…æ²»ç†æ©Ÿåˆ¶èˆ‡ç¨½æ ¸æµç¨‹ï¼Œæ‰èƒ½é•·æœŸç¶­æŒåˆè¦æ€§èˆ‡å¯è§£é‡‹æ€§ã€‚",
    ]

    human_like_en = [
        "I remember walking to class half asleep and spilling coffee everywhere because I forgot to put the lid on.",
        "My grandma tells stories with little details about the smells in her kitchen and the creaky porch swing.",
        "Sometimes I rewrite the same sentence five times before it feels right, and other days I just give up.",
        "The bus was late again, so everyone at the stop started sharing weather complaints like old friends.",
        "I tried three recipes before finally admitting I just wanted noodles with butter and too much pepper.",
        "My friend texted me a meme during the meeting and I had to stare at the ceiling to stop laughing.",
        "There is a crooked tree on my street that always looks like it's waving when the wind picks up.",
        "The concert was so loud my jacket vibrated, but the encore was worth the ringing ears.",
        "Yesterday I biked through the park and nearly collided with a family of ducks crossing the path.",
        "I wrote this paragraph while waiting for laundry to finish, hoping the dryer wouldn't eat a sock.",
        "We argued about which movie to watch longer than the movie itself would have lasted.",
        "The night smelled like rain and sunscreen after a long day at the beach."
    ]

    human_like_zh = [
        "æˆ‘åœ¨ä¾¿åˆ©å•†åº—è²·äº†å’–å•¡ï¼Œå»åœ¨å‡ºé–€æ™‚æ‰“ç¿»ï¼Œæ•´æ¢è·¯éƒ½èå¾—åˆ°ç”œç”œçš„å‘³é“ã€‚",
        "æœ‹å‹å‚³ä¾†ä¸€å¼µè²“å’ªæ¢—åœ–ï¼Œæˆ‘åœ¨æ·é‹ä¸Šå¿ç¬‘åˆ°çœ¼æ·šéƒ½é£†å‡ºä¾†ï¼Œæ—é‚Šçš„äººä¸€é ­éœ§æ°´ã€‚",
        "é€±æœ«å’Œçˆ¸åª½ç…®ç«é‹ï¼Œçµæœé’èœè²·å¤ªå¤šï¼Œæœ€å¾Œå¤§å®¶ç¡¬è‘—é ­çš®æŠŠå®ƒå€‘åƒå®Œã€‚",
        "æ˜¨æ™šä¸‹ç­é¨è»Šå›å®¶æ™‚çªç„¶é–‹å§‹é£„é›¨ï¼Œè·¯é‚Šçš„éœ“è™¹ç‡ˆè¢«æ‰“æ¿•å¾Œçœ‹èµ·ä¾†æ¨¡ç³Šåˆæ¼‚äº®ã€‚",
        "æˆ‘å¸¸å¸¸æŠŠç­†è¨˜æœ¬å¯«åˆ°ä¸€åŠå°±åœä¸‹ä¾†ï¼Œå› ç‚ºè²“è·³åˆ°æ¡Œä¸ŠæŠŠå¢¨æ°´å¼„å¾—äº‚ä¸ƒå…«ç³Ÿã€‚",
        "éš”å£çš„å°å­©ç·´é‹¼ç´èµ°éŸ³ï¼Œä½†è½ä¹…äº†åè€Œæœ‰é»ç¿’æ…£ï¼Œåƒæ˜¯æ¯æ—¥çš„èƒŒæ™¯éŸ³æ¨‚ã€‚",
    ]

    ai_like = ai_like_en + ai_like_zh
    human_like = human_like_en + human_like_zh
    texts = ai_like + human_like
    labels = [1] * len(ai_like) + [0] * len(human_like)  # 1=AI-like, 0=Human-like
    return texts, np.array(labels)


@st.cache_resource
def load_model():
    texts, labels = build_dataset()
    pipeline = Pipeline(
        [
            # Char n-grams coverä¸­è‹±æ–‡æ··åˆï¼Œé¿å…æ–·è©å·®ç•°
            ("tfidf", TfidfVectorizer(analyzer="char", ngram_range=(3, 5), max_features=6000)),
            ("clf", LogisticRegression(max_iter=1000)),
        ]
    )
    pipeline.fit(texts, labels)
    return pipeline


def get_feature_weights(model, top_k=10):
    """Return the top positive (AI) and negative (human) n-grams."""
    tfidf = model.named_steps["tfidf"]
    clf = model.named_steps["clf"]
    feature_names = tfidf.get_feature_names_out()
    coefs = clf.coef_[0]
    top_ai = sorted(zip(feature_names, coefs), key=lambda x: x[1], reverse=True)[:top_k]
    top_human = sorted(zip(feature_names, coefs), key=lambda x: x[1])[:top_k]
    return top_ai, top_human


def compute_text_stats(text: str):
    tokens = re.findall(r"[A-Za-z\u4e00-\u9fff']+", text)
    sentences = [s for s in re.split(r"[.!?ã€‚ï¼ï¼Ÿ]", text) if s.strip()]
    punctuation = re.findall(r"[.,;:!?ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š]", text)
    word_count = len(tokens)
    unique_ratio = len(set(tokens)) / word_count if word_count else 0
    avg_word_len = np.mean([len(t) for t in tokens]) if tokens else 0
    sentence_len = word_count / len(sentences) if sentences else word_count
    punct_density = len(punctuation) / max(len(text), 1)
    return {
        "words": word_count,
        "sentences": len(sentences),
        "avg_word_len": round(avg_word_len, 2),
        "unique_ratio": round(unique_ratio * 100, 1),
        "words_per_sentence": round(sentence_len, 1),
        "punct_density": round(punct_density * 100, 2),
    }


def prob_chart(human_pct: float, ai_pct: float):
    data = pd.DataFrame(
        {"label": ["Human", "AI"], "probability": [human_pct, ai_pct], "color": ["#34bfa3", "#4c6fff"]}
    )
    chart = (
        alt.Chart(data)
        .mark_bar(cornerRadiusTopLeft=6, cornerRadiusTopRight=6)
        .encode(
            x=alt.X("probability:Q", axis=alt.Axis(format=".0f", title="Probability (%)")),
            y=alt.Y("label:N", sort=None, title=None),
            color=alt.Color("color:N", scale=None),
            tooltip=["label", alt.Tooltip("probability:Q", format=".1f")],
        )
        .properties(height=120)
    )
    st.altair_chart(chart, use_container_width=True)


def feature_chart(features, title):
    df = pd.DataFrame(features, columns=["feature", "weight"])
    chart = (
        alt.Chart(df)
        .mark_bar(cornerRadiusTopLeft=4, cornerRadiusTopRight=4)
        .encode(
            x=alt.X("weight:Q", title="weight"),
            y=alt.Y("feature:N", sort="-x", title=None),
            tooltip=["feature", alt.Tooltip("weight:Q", format=".3f")],
            color=alt.value("#ffaa33"),
        )
        .properties(title=title, height=240)
    )
    st.altair_chart(chart, use_container_width=True)


def inject_style():
    st.markdown(
        """
        <style>
            .main {
                background: radial-gradient(circle at 10% 20%, rgba(76,111,255,0.08), transparent 25%),
                            radial-gradient(circle at 80% 0%, rgba(52,191,163,0.12), transparent 20%),
                            linear-gradient(135deg, #0d1117, #111827);
                color: #e8eef6;
            }
            .block-container {
                padding-top: 2rem;
                padding-bottom: 2rem;
                max-width: 1000px;
            }
            .glass {
                background: rgba(255, 255, 255, 0.04);
                border: 1px solid rgba(255, 255, 255, 0.08);
                border-radius: 14px;
                padding: 1.25rem 1.4rem;
                box-shadow: 0 14px 40px rgba(0,0,0,0.25);
            }
            h1, h2, h3, h4 {
                letter-spacing: 0.4px;
            }
            .stTextArea textarea {
                border-radius: 12px;
            }
        </style>
        """,
        unsafe_allow_html=True,
    )


def main():
    st.set_page_config(page_title="AI vs Human Detector", page_icon="ğŸ¤–", layout="wide")
    inject_style()

    st.title("AI vs Human Detector")
    st.caption("Enter English or Chinese text â†’ instant AI% / Human% with model weights and language stats.")

    hero = st.container()
    with hero:
        st.markdown(
            "<div class='glass'>Lightweight tf-idf + Logistic Regression; demo-trained on a tiny bilingual corpus for quick checks.</div>",
            unsafe_allow_html=True,
        )

    model = load_model()

    default_text = (
        "This draft summarizes the experiment and concludes with recommendations for deployment."
    )
    user_text = st.text_area("Enter text", default_text, height=220)

    if user_text.strip():
        proba = model.predict_proba([user_text.strip()])[0]
        human_pct = float(proba[0] * 100)
        ai_pct = float(proba[1] * 100)

        st.subheader("Detection results")
        col1, col2, col3 = st.columns([1, 1, 1])
        col1.metric("Human%", f"{human_pct:.1f}%", delta=f"{human_pct - ai_pct:+.1f}% vs AI")
        col2.metric("AI%", f"{ai_pct:.1f}%")
        col3.metric("Confidence gap", f"{abs(ai_pct - human_pct):.1f} pts")

        prob_chart(human_pct, ai_pct)

        stats = compute_text_stats(user_text)
        st.subheader("Language stats")
        c1, c2, c3 = st.columns(3)
        c4, c5, c6 = st.columns(3)
        c1.metric("Words", f"{stats['words']}")
        c2.metric("Sentences", f"{stats['sentences']}")
        c3.metric("Avg word len", f"{stats['avg_word_len']}")
        c4.metric("Lexical diversity", f"{stats['unique_ratio']}%")
        c5.metric("Avg sentence len", f"{stats['words_per_sentence']}")
        c6.metric("Punctuation density", f"{stats['punct_density']}%")

        st.caption("Scores are indicative only; please pair with human review and larger datasets.")

    st.divider()
    st.subheader("Model info & keywords")
    texts, labels = build_dataset()
    st.write(f"Training samples: {len(texts)} (AI-like {labels.sum()} / Human-like {(labels == 0).sum()})")
    top_ai, top_human = get_feature_weights(model)

    col_a, col_b = st.columns(2)
    with col_a:
        feature_chart([(f, w) for f, w in top_ai], "AI-leaning n-grams")
    with col_b:
        feature_chart([(f, abs(w)) for f, w in top_human], "Human-leaning n-grams")

    st.caption("To improve accuracy: use larger real datasets, external LM features, or persist a trained model to skip retraining.")


if __name__ == "__main__":
    main()
